{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4장 신경망 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 손실함수 값을 가급적이면 가장 작게 만드는 기법! : 경사법\n",
    "- 오차제곱합, 교차 엔트로피"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 오차제곱합\n",
    "- 원 핫 인코딩 : 한 원소만 1, 나머지는 다 0으로 표기하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]\n",
    "t=[0,0,1,0,0,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_squares_error(y,t):\n",
    "    return 0.5*np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 편미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)  # x와 형상이 같은 배열을 생성\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "        x[idx] = tmp_val  # 값 복원\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 8.]\n",
      "[0. 4.]\n",
      "[6. 0.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))  # [ 6.  8.]\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))  # [ 0.  4.]\n",
    "print(numerical_gradient(function_2, np.array([3.0, 0.0])))  # [ 6.  0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변수가 2개일때 어느 변수에 대한 미분이냐를 구분해야하고, 덧붙여 이와같이 변수가 여럿인 함수에 대한 미분을 편미분이라고 한다.  \n",
    "편미분은 변수가 하나인 미분과 마찬가지로 특정 장소의 기울기를 구한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 경사법(경사 하강법)\n",
    "기계학습 문제 대부분은 학습단계에서 최적의 매개변수를 찾아낸다. 신경망 역시 최적의 매개변수(가중치와 편향)를 학습시에 찾아야 한다.  \n",
    "최적 : 손실함수가 최솟값이 될 때의 매개변수 값!  \n",
    "기울기를 잘 이용해 함수의 최솟값을 찾으려는 것이 경사법!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " x0 = x0 - η*∂f/∂x0  \n",
    " x1 = x1 - η*∂f/∂x1  \n",
    " η(eta) : 갱신하는 양, 학습률learning rate  \n",
    " 위 식을 반복  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f:최적화하려는 함수\n",
    "# init_x : 초깃값\n",
    "# lr : 학습률\n",
    "# step_num : 반복횟수\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    x_history = []\n",
    "\n",
    "    for i in range(step_num):\n",
    "        x_history.append(x.copy())\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x, np.array(x_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.11110793e-10  8.14814391e-10]\n"
     ]
    }
   ],
   "source": [
    "# 경사법으로 f(x0, x1) = x0² + x1²의 최솟값을 구해라\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "x, x_history = gradient_descent(function_2, init_x, lr=0.1)\n",
    "print(x)  # [ -6.11110793e-10   8.14814391e-10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.58983747e+13 -1.29524862e+12]\n"
     ]
    }
   ],
   "source": [
    "# 학습률이 너무 큼\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "x, x_history = gradient_descent(function_2, init_x, lr=10.0)\n",
    "print(x)  # [ -2.58983747e+13  -1.29524862e+12] 발산함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.99999994  3.99999992]\n"
     ]
    }
   ],
   "source": [
    "# 학습률이 너무 작음\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "x, x_history = gradient_descent(function_2, init_x, lr=1e-10)\n",
    "print(x)  # [-2.99999994  3.99999992] 거의 변화 없음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습률이 너무 크다면 발산해버리고, 너무 작아버리면 갱신되지 않은채 끝나버린다. 따라서 적절한 학습률이 중요함!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터\n",
    "학습률 같은 매개변수를 하이퍼파라미터라고 한다. 이는 가중치와 편향같은 신경망의 매개변수와는 성질이 다른 매개변수!!     \n",
    "\n",
    "신경망의 가중치 매개변수는 훈련데이터와 학습알고리즘에 의해서 '자동'으로 획득되는 매개변수인 반면, 학습률같은 하이퍼파라미터는  \n",
    "사람이 직접 설저해야 하는 매개변수인 것이다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 그래프\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "x, x_history = gradient_descent(function_2, init_x, lr=0.1, step_num=20)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], '--b')\n",
    "plt.plot([0, 0], [-5, 5], '--b')\n",
    "plt.plot(x_history[:, 0], x_history[:, 1], 'o')\n",
    "\n",
    "plt.xlim(-3.5, 3.5)\n",
    "plt.ylim(-4.5, 4.5)\n",
    "plt.xlabel(\"X0\")\n",
    "plt.ylabel(\"X1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 학습 알고리즘 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''\n",
    "전제 : 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 적응하도록 조정하는 과정을 '학습'이라 한다.  \n",
    "신경망 학습은 다음과 같이 4단계로 수행한다.  \n",
    "1단계 - 미니배치  \n",
    "훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라 하며,\n",
    "그 미니배치의 손실함수 값을 줄이는 것이 목표이다.  \n",
    "2단계 - 기울기 산출  \n",
    "미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다.\n",
    "기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.  \n",
    "3단계 - 매개변수 갱신  \n",
    "가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다.    \n",
    "4단계 - 반복  \n",
    "1~3단계를 반복한다.  \n",
    "데이터를 무작위로 선정하기 때문에 확률적 경사 하강법stochastic gradient descent,\n",
    "SGD라고 부른다.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.append(os.pardir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 에폭(epoch) 은 하나의 단위\n",
    "시험데이터로 평가할때 1에폭은 학습에서 훈련데이터를 모두 소진했을 때의 회수에 해당한다.  \n",
    "예를들어 훈련 10,000개를 100개의 미니배치로 학습할 경우, 확률적 경사 하강법을 100회 반복하면 모든 훈련 데이터를 '소진'한 게 된다.  \n",
    "이 경우 100회가 1에폭이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
